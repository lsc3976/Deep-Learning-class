{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPihRfdj9WXMaHRwK4rQL/b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Setting up your optimization problem"],"metadata":{"id":"3H1t7WMH7Dk0"}},{"cell_type":"markdown","source":["- - -"],"metadata":{"id":"ze7Jmmwt7Jq0"}},{"cell_type":"markdown","source":["##Gradient Checking"],"metadata":{"id":"sde2s44v7KoD"}},{"cell_type":"markdown","source":["- - -"],"metadata":{"id":"381ePgiz7qT8"}},{"cell_type":"markdown","source":["###학습목표"],"metadata":{"id":"Yer9US1o7oXU"}},{"cell_type":"markdown","source":["- - -"],"metadata":{"id":"-rqM-QUf7p20"}},{"cell_type":"markdown","source":["경사 검사는 시간을 절약하고 backward propogation 의 구현에 대한 버그를 찾는데\n","도움을 많이 준다. 어떻게  경사 검사를 할 수 있는지 알아보자."],"metadata":{"id":"KSnrxXHF7uWk"}},{"cell_type":"markdown","source":["- - -"],"metadata":{"id":"cCbcQq5-7-Js"}},{"cell_type":"markdown","source":["### Gradient check 의 순서"],"metadata":{"id":"MGZHfixx8kcs"}},{"cell_type":"markdown","source":["- - -"],"metadata":{"id":"uJEh-Spr82NF"}},{"cell_type":"markdown","source":["  　　 $W^{[1]}$,$b^{[1]}$,$\\cdots$,$W^{[L]}$,$b^{[L]}$"],"metadata":{"id":"orCqkG639_xE"}},{"cell_type":"markdown","source":["1. 행렬 $W^{[1]}$을 벡터로 크기를 바꾼다.\n","\n","2. 모든 W 행렬을 받아서 벡터로 바꾸고 모두 연결시킨다.\n","\n","3. 그결과 매우 큰 벡터 매개변수 θ 를 얻게된다.\n","\n","4. 비용함수 J를 W와 b의 함수로 만드는 대신에 θ의 함수가 되도록 한다.\n"],"metadata":{"id":"IABjCfDZ83Jk"}},{"cell_type":"markdown","source":["- - -"],"metadata":{"id":"MwbdEjUW-Xwf"}},{"cell_type":"markdown","source":["　　d$W^{[1]}$,d$b^{[1]}$,$\\cdots$,d$W^{[L]}$,d$b^{[L]}$"],"metadata":{"id":"anxTxIT1-Mz0"}},{"cell_type":"markdown","source":["1. d$W^{[1]}$ 을 벡터로 바꾼다.  d$b^{[1]}$는 이미 벡터이다.\n","\n","2. 모든 dW 는 행렬이다. d$W^{[1]}$ 은 $W^{[1]}$과 같은 차원이고  d$b^{[1]}$ 은 $b^{[1]}$과 같은 차원이다. \n","\n"," 같은 방식으로 크기를 바꾸고 연결하여 모든 미분값을 매우 큰 벡터 dθ로 바꿀 수 있다.\n","\n"," $\\rightarrow$여기서 dθ또한 θ와 같은 차원이다.\n"],"metadata":{"id":"Nv5XaHob-hSl"}},{"cell_type":"markdown","source":["### Gradient check(Grad check)"],"metadata":{"id":"AtU_SrK5AD4m"}},{"cell_type":"markdown","source":["- - -"],"metadata":{"id":"h-jNOGdAALr1"}},{"cell_type":"markdown","source":["for each i :\n","\n","　d$θ_{approx}$[i] = $\\frac{J(θ_{1},θ_{2},\\cdots,θ_i+ε,\\cdots)-J (θ_{1},θ_{2},\\cdots,θ_i+ε,\\cdots)}{2ε}$\n","\n","　　　　　 $\\approx$ dθ[i]=$\\frac{\\partial J }{\\partial θ_{i}}$"],"metadata":{"id":"p4aE0WPQAZU9"}},{"cell_type":"markdown","source":["###d$θ_{approx}$ $\\approx$ dθ 확인하는 방법"],"metadata":{"id":"H7PvuYM3GUgT"}},{"cell_type":"markdown","source":["- - -"],"metadata":{"id":"cJVPVNTwGjfY"}},{"cell_type":"markdown","source":["1. d$θ_{approx}$ 와 dθ의 유클리드 거리를 계산한다.\n","\n","\n","2. 벡터의 길이로 정규화 하기 위해 lld$θ_{approx}$ll+lldθll의 유클리드 길이로 나눠준다.\n"," \n","  이 벡터가 아주 작거나 큰 경우에 대비해 분모는 이 식을 비율로 바꾼다.\n","\n","　　　if ε를 $10^{-7}$로 사용하면은\n","이범위의 ε에서  $\\frac{\\left \\| dθ_{approx} -dθ \\right \\|_{2}}{\\left \\| dθ_{approx}\\right \\|_{2}+\\left \\| dθ\\right \\|_{2}}$  수식은 $10^{-7}$이나 그보다 작은 결과가 된다.\n","\n","\n","\n","3. Check  　　 $\\frac{\\left \\| dθ_{approx} -dθ \\right \\|_{2}}{\\left \\| dθ_{approx}\\right \\|_{2}+\\left \\| dθ\\right \\|_{2}}$  $\\approx$ $10^{-7}$ - great\n","\n"," 　　　　　　　　　　　　　　 $10^{-5}$\n","\n","\n","　　　　　　　　　　　　　　　　　$10^{-3}$- worry\n","\n","4. 수식이  $10^{-5}$ 라면 괜찮긴 하지만 너무 큰 원소가 있는 것은 아닌지 벡터의 원소를 이중으로 확인해 봐야한다. 왜냐하면 원소의 차이가 너무 크다면 버그가 있을 수도 있기 때문이다.\n","\n"],"metadata":{"id":"SRKfbWVyGkmQ"}},{"cell_type":"markdown","source":["###결론"],"metadata":{"id":"L2W9AxZpZaR1"}},{"cell_type":"markdown","source":["신경망의 Forward propogation 또는 Backward propogation 구현할 때 경사 검사에서 상대적으로 큰 값이 나온다면 버그의 가능성을 의심해야한다. 디버깅의 과정을 거친 후 경사 검사에서 작은 값이 나온다면 구현에 대한 자신감을 가져도 좋다."],"metadata":{"id":"JL8rcHI7ZbuV"}}]}