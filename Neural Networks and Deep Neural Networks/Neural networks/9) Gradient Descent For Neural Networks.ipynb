{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient Descent For Neural Networks (9강).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNbbDiTbVbVP0oIAVV76zNs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#one hidden layer Neural Network"],"metadata":{"id":"r5nINuEVVvId"}},{"cell_type":"markdown","source":["##Gradient Descent For Neural Networks\n","- - -"],"metadata":{"id":"MYoi86OM91jp"}},{"cell_type":"markdown","source":["###1.학습목표\n","_ _ _"],"metadata":{"id":"hH2ecnh7VxAl"}},{"cell_type":"markdown","source":["* 한 개의 은닉층을 가진 신경망의 경사 하강법을 구현하는 방법 알아보기.\n","* 역전파 계산하기.\n","\n","* 경사 하강법을 적용하기 위한 식들 알아보기.\n","\n","* 왜 이런 물리적인 식들이 신경망에서 경사를 계산하는데 있어서 올바른 식인지 알아보기.\n"],"metadata":{"id":"P0Aul8vIV0kV"}},{"cell_type":"markdown","source":["###2. 단일 신경망 구조 remind\n","___"],"metadata":{"id":"Cmw1Zy9yqubN"}},{"cell_type":"markdown","source":["현재 우리의 단일층 신경망은 $n_{x}$=$n^{[0]}$(입력 특성),$n^{[1]}$(은닉 유닛),$n^{[2]}$(출력 유닛) 이고 $n^{[2]}$가 1일때를 나타낸다.\n","\n","* Parameters:$w^{[1]}$ ,$b^{[1]}$,$w^{[2]}$,$b^{[2]}$ \n","\n","\n","* **$w^{[1]}$** 의 차원은 ($n^{[1]},1$)\n","\n","* $b^{[1]}$의 차원은($n^{[2]},n^{[0]}$)\n","\n","* $w^{[2]}$의 차원은($n^{[2]},n^{[1]}$)\n","\n","* $b^{[2]}$의 차원은($n^{[2]},1$)\n","\n","\n","\n"],"metadata":{"id":"zJeF2SNHbRXj"}},{"cell_type":"markdown","source":["<단일층 신경망에서 이진 분류를 하고 있다고 가정하면?>\n","\n","cost function: J($w^{[1]}$,$b^{[1]}$,$w^{[2]}$, $b^{[2]}$) =$\\frac{1}{m}$ $\\sum_{i=1}^{m}$ L( $\\hat{y}$ , y)\n","\n","* L은 신경망이 예측한  $\\hat{y}$값에 대한 손실이다.\n","\n","*  $\\hat{y}$ 은 $a^{[2]}$ 이다.\n","\n","* y값은 정답 레이블이다.\n","\n","\n","* 손실 함수는 로지스틱 회귀에서 사용한 것과 같다.\n","\n","\n"],"metadata":{"id":"BFOfpj7ye3Ak"}},{"cell_type":"markdown","source":["###3.단일층 신경망에서 경사하강법 순서\n","___"],"metadata":{"id":"QSRC0ngopNXe"}},{"cell_type":"markdown","source":["\n","\n","i.신경망을 훈련시킬 때 0이 아닌 값으로 변수를 초기화한다.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"2r33gmbSiVk7"}},{"cell_type":"markdown","source":["ii.경사하강법이 반복될 때마다 예측값을 계산한다. 그다음 도함수를 계산한다.\n","\n"," compute predicted value ( $\\hat{y}^{(i)}$ , i=1$\\cdots$m)\n","\n"," * $dw_1=\\frac{dJ}{dw_1}$\n"," \n","*  $db_1=\\frac{dJ}{db_1}$\n","\n","* $dw_2=\\frac{dJ}{dw_2}$\n","\n","* $db_2=\\frac{dJ}{db_2}$\n","\n","* $w_1$ :=$w_1$ - $\\alpha$ d$w_1$\n","\n","* $w_2$ :=$w_2$ - $\\alpha$ d$w_2$\n","\n","\n"," * $b:= $b- $\\alpha$ d$b$ \n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"OdLtdtY-jEC7"}},{"cell_type":"markdown","source":["iii.변수들이 수렴할 때 까지 아래과정을 반복한다.\n","\n","Repeat {\n",">>  compute predicted value ( $\\hat{y}^{(i)}$ , i=1$\\cdots$m)\n","\n"," >>  $dw_1=\\frac{dJ}{dw_1}$\n","> >,$db_1=\\frac{dJ}{db_1}$\n","> ,$dw_2=\\frac{dJ}{dw_2}$\n",",$db_2=\\frac{dJ}{db_2}$\n","\n",">> $w_1$ :=$w_1$ - $\\alpha$ d$w_1$\n","\n",">>$w_2$ :=$w_2$ - $\\alpha$ d$w_2$\n","\n","\n",">> $b:= $b- $\\alpha$ d$b$ }\n"],"metadata":{"id":"trTASai0n7SL"}},{"cell_type":"markdown","source":["###4.Formulas for computing derivatives\n","_ _ _\n","\n","\n","\n"],"metadata":{"id":"jWwjFMUarJNN"}},{"cell_type":"markdown","source":["$\\mathbf{Forma}$   $\\mathbf{ propagation :}$\n","\n","* = $W^{[1]}$X+$b^{[1]}$ \n","\n","* $A^{[1]}$= $g^{[1]}(Z^{[1]}$)\n","\n","* $Z^{[2]}$=$W^{[1]}$$A^{[1]}$+$b^{[2]}$ \n","\n","\n","* $A^{[2]}$= $g^{[2]}(Z^{2]}$)=$\\sigma$($Z^{[2]}$)$\\rightarrow$ 여기서는 이진 분류를 하고 있기 때문에 이 활성화 함수는 실제로는 시그모이드 함수가 된다.\n"],"metadata":{"id":"xiPWfEuvsIY1"}},{"cell_type":"markdown","source":["$\\mathbf{Back}$   $\\mathbf{ propagation 식들 :}$\n","\n","\n"," * d$Z^{[2]}$ = $A^{[2]}$ \n","\n","* d$W^{[2]}$=$\\frac{1}{m}$d$Z^{[2]}$ $A^{[1]T}$\n","\n","* d$b^{[2]}$=$\\frac{1}{m}$np.sum(d$Z^{[2]}$ ,axis=1,keepdims=True)\n","\n","   * np.sum은 행렬의 어떤 축 방향으로 덧셈을 계산 할때 사용한다.\n"," \n","   * np.sum은 행렬에서 axis=1일경우에는 가로로 더한다.\n"," \n","   * keepdims는 파이썬이 잘못된 1차원 배열을 출력하지 않게 하는 것이다.\n","\n","   * keepdims를 사용하면 d$b^{[2]}$의 차원이 ($n^{[2]}$,1)이된다.\n","\n","\n","\n","\n","* d$Z^{[1]}$=$W^{[2]T}$d$Z^{[2]}$$\\times$${g^{[1]}}'$($Z^{[1]}$)\n","\n","  * ${g^{[1]}}'$은 은닉층에서 사용했던 활성화 함수의 도함수이다.\n","\n","  * **여기서'$\\times$'는 요소별 곱셈이다**.\n","\n","* d$W^{[1]}$=$\\frac{1}{m}$d$Z^{[1]}$$X^T$\n","\n","* d$b^{[1]}$=$\\frac{1}{m}$np.sum(d$Z^{[1]}$ ,axis=1,keepdims=True)\n","\n","   * keepdims대신 사용할수있는 함수는 reshape 함수이다.\n"," \n","\n","\n","\n","\n"],"metadata":{"id":"th_dB4hQwUTF"}},{"cell_type":"markdown","source":["####5.참조\n","- - - "],"metadata":{"id":"yidkP_C2OH-D"}},{"cell_type":"markdown","source":["https://www.youtube.com/watch?v=7bLEWDZng_M&t=292s"],"metadata":{"id":"9EhTeno3OZE7"}}]}